from abc import ABC, abstractmethod
from typing import List, Optional, Union
import structlog
from openai import AsyncOpenAI, OpenAIError
from fastembed import TextEmbedding

from orgmind.platform.config import settings

logger = structlog.get_logger()

class EmbeddingProvider(ABC):
    """Abstract base class for embedding providers."""

    @property
    @abstractmethod
    def dimension(self) -> int:
        """Return the dimension of the embeddings generated by this provider."""
        pass

    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        """Generate an embedding for a single text string."""
        pass

    @abstractmethod
    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a batch of text strings."""
        pass

class OpenAIEmbeddingProvider(EmbeddingProvider):
    """OpenAI implementation of EmbeddingProvider."""

    def __init__(self, api_key: str, model: str = "text-embedding-3-small"):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        # Dimensions for common OpenAI models
        self._dimensions = {
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072,
            "text-embedding-ada-002": 1536
        }

    @property
    def dimension(self) -> int:
        return self._dimensions.get(self.model, 1536)

    async def embed(self, text: str) -> List[float]:
        """Generate an embedding for a single text string."""
        if not text:
            return [0.0] * self.dimension
            
        try:
            # Replace newlines which can negatively affect performance
            text = text.replace("\n", " ")
            response = await self.client.embeddings.create(
                input=[text],
                model=self.model
            )
            return response.data[0].embedding
        except OpenAIError as e:
            logger.error("openai_embedding_failed", error=str(e))
            raise

    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a batch of text strings."""
        if not texts:
            return []
            
        try:
            # Replace newlines for all texts
            cleaned_texts = [t.replace("\n", " ") for t in texts]
            response = await self.client.embeddings.create(
                input=cleaned_texts,
                model=self.model
            )
            # Ensure order is preserved (OpenAI usually preserves order)
            return [data.embedding for data in response.data]
        except OpenAIError as e:
            logger.error("openai_embedding_batch_failed", error=str(e))
            raise

class FastEmbedProvider(EmbeddingProvider):
    """Local implementation of EmbeddingProvider using fastembed (ONNX)."""

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = TextEmbedding(model_name=model_name)
        # FastEmbed handles tokenization and inference locally via ONNX Runtime
        
    @property
    def dimension(self) -> int:
        # FastEmbed models are usually initialized, but we can inspect manually or trust known dims
        # all-MiniLM-L6-v2: 384
        # bge-small-en-v1.5: 384
        # We can inspect the underlying model config if needed, but for now we'll query the instance
        # if possible, or fallback to knowns.
        # FastEmbed doesn't easily expose dim property on the wrapper? 
        # Actually it does expose logic to get embeddings.
        # We can do a dummy run or check docs.
        # Let's rely on a check at init time or property.
        # Since it's synchronous INIT, we can do a dummy embed of "typcial" to find out.
        # OR just hardcode common ones or trust the user config aligned with expectations.
        # To be safe:
        try:
             # FastEmbed wrapper usually doesn't expose dimension directly as a property easily.
             # But we can get it from the model itself potentially.
             # Let's verify with a dummy embed once.
             # Optimization: Cache it
             if not hasattr(self, "_dim"):
                 # Determine dimension via dummy
                 # This is cheap in FastEmbed
                 dummy = list(self.model.embed(["test"]))
                 self._dim = len(dummy[0])
             return self._dim
        except Exception:
             return 384 # Fallback for MiniLM

    async def embed(self, text: str) -> List[float]:
        # FastEmbed is synchronous (CPU bound), but creating thread might be overkill for single text
        # unless under heavy load.
        # However, to keep our API async friendly, we should probably run this in an executor
        # if we expect it to take > 10-20ms.
        # For a single short text it's fast. For batches, it might block the event loop.
        # We'll run it directly for now as FastEmbed is... fast. 
        # Actually, fastembed is generator based.
        
        if not text:
            return [0.0] * self.dimension
            
        # Helper to run iterator
        # We wrap in list() to get results
        embeddings = list(self.model.embed([text]))
        return embeddings[0].tolist()

    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []
            
        # Generator to list
        # For large batches, this blocks. 
        # In a real async app we might want loop.run_in_executor
        embeddings = list(self.model.embed(texts))
        return [e.tolist() for e in embeddings]

class MockEmbeddingProvider(EmbeddingProvider):
    """
    Mock provider for testing or when no API key is present.
    Generates deterministic pseudo-random embeddings seeded by input text length.
    """
    
    def __init__(self, dimension: int = 1536):
        self._dimension = dimension
        
    @property
    def dimension(self) -> int:
        return self._dimension

    async def embed(self, text: str) -> List[float]:
        # Simple deterministic "embedding" for testing
        # We use a simple hash-like mechanism to generate non-zero vector
        seed = sum(ord(c) for c in text) if text else 0
        vector = []
        for i in range(self.dimension):
            # Generate a value between -1 and 1
            val = ((seed + i) % 100) / 50.0 - 1.0
            vector.append(val)
        return vector

    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        results = []
        for text in texts:
            results.append(await self.embed(text))
        return results

def get_embedding_provider() -> EmbeddingProvider:
    """Factory to get the configured embedding provider."""
    
    provider_type = settings.EMBEDDING_PROVIDER.lower()
    
    if provider_type == "openai":
        if not settings.OPENAI_API_KEY:
            logger.warning("openai_provider_requested_but_no_key_falling_back_to_mock")
            return MockEmbeddingProvider()
            
        logger.info("using_openai_embeddings", model=settings.OPENAI_EMBEDDING_MODEL)
        return OpenAIEmbeddingProvider(
            api_key=settings.OPENAI_API_KEY,
            model=settings.OPENAI_EMBEDDING_MODEL
        )
    
    elif provider_type == "local":
        logger.info("using_local_embeddings", model=settings.LOCAL_EMBEDDING_MODEL)
        try:
            return FastEmbedProvider(model_name=settings.LOCAL_EMBEDDING_MODEL)
        except Exception as e:
            logger.error("failed_to_init_fastembed_falling_back_to_mock", error=str(e))
            return MockEmbeddingProvider()

    # Fallback/Default
    logger.warning("unknown_provider_using_mock", provider=provider_type)
    return MockEmbeddingProvider()
